"""
–£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è >92% accuracy

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
- 10 –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ 5
- –õ—É—á—à–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: ResNet-like
- –†–∞–∑–Ω—ã–µ seeds –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- –õ–µ–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
- Longer training —Å early stopping
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import json
import medmnist
from medmnist import INFO


def set_seed(seed):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f'Using CUDA: {torch.cuda.get_device_name(0)}')
elif torch.backends.mps.is_available():
    device = torch.device('mps')
    print('Using Apple Metal (MPS)')
else:
    device = torch.device('cpu')
    print('Using CPU')

print(f'PyTorch version: {torch.__version__}\n')


# ===========================================================================
# –õ—É—á—à–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - ResNet-like
# ===========================================================================

class ResidualBlock(nn.Module):
    """Residual block"""
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = F.relu(out)
        return out


class ResNetLikeCNN(nn.Module):
    """ResNet-like —Å skip connections"""
    def __init__(self, num_classes=11, dropout=0.3, hidden_dim=64):
        super(ResNetLikeCNN, self).__init__()

        self.conv1 = nn.Conv2d(1, hidden_dim, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.pool = nn.MaxPool2d(2, 2)

        self.res_block1 = ResidualBlock(hidden_dim)
        self.res_block2 = ResidualBlock(hidden_dim)

        self.conv2 = nn.Conv2d(hidden_dim, 128, 3, padding=1, stride=2)
        self.bn2 = nn.BatchNorm2d(128)

        self.global_pool = nn.AdaptiveAvgPool2d(1)

        self.fc = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)

        x = self.res_block1(x)
        x = self.res_block2(x)

        x = F.relu(self.bn2(self.conv2(x)))

        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x


# ===========================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ===========================================================================

data_flag = 'organcmnist'
info = INFO[data_flag]
NUM_CLASSES = len(info['label'])
BATCH_SIZE = 128

print('=' * 70)
print('–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ OrganCMNIST')
print('=' * 70)

# –õ–µ–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è val/test
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

DataClass = getattr(medmnist, info['python_class'])

val_dataset = DataClass(split='val', download=False, transform=test_transform, as_rgb=False)
test_dataset = DataClass(split='test', download=False, transform=test_transform, as_rgb=False)

val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f'Val samples: {len(val_dataset)}')
print(f'Test samples: {len(test_dataset)}\n')


# ===========================================================================
# –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
# ===========================================================================

def train_epoch(model, dataloader, criterion, optimizer, device):
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(dataloader, desc='Training', leave=False):
        images, labels = images.to(device), labels.to(device).squeeze().long()

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return running_loss / total, 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc='Evaluating', leave=False):
            images, labels = images.to(device), labels.to(device).squeeze().long()

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / total, 100. * correct / total


def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer,
                                     num_epochs, device, patience=15, verbose=False):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å early stopping"""
    best_val_acc = 0.0
    patience_counter = 0

    for epoch in range(num_epochs):
        if verbose and epoch % 5 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}')

        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            if verbose:
                print(f'Early stopping at epoch {epoch+1}')
            break

    return best_val_acc


def get_predictions_proba(model, dataloader, device):
    """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc='Getting predictions', leave=False):
            images = images.to(device)
            outputs = model(images)
            probs = F.softmax(outputs, dim=1)

            all_probs.append(probs.cpu().numpy())
            all_labels.append(labels.squeeze().numpy())

    return np.vstack(all_probs), np.concatenate(all_labels)


# ===========================================================================
# –û–±—É—á–µ–Ω–∏–µ —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è (10 –º–æ–¥–µ–ª–µ–π)
# ===========================================================================

print('=' * 70)
print('–û–ë–£–ß–ï–ù–ò–ï –£–í–ï–õ–ò–ß–ï–ù–ù–û–ì–û –ê–ù–°–ê–ú–ë–õ–Ø (10 –ú–û–î–ï–õ–ï–ô)')
print('=' * 70)
print()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è - 10 –º–æ–¥–µ–ª–µ–π
ensemble_config = [
    # –†–∞–∑–Ω—ã–µ learning rates
    {'name': 'ResNet-LR0.002-1', 'seed': 42, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 64},
    {'name': 'ResNet-LR0.002-2', 'seed': 52, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 64},
    {'name': 'ResNet-LR0.002-3', 'seed': 62, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 64},

    # –†–∞–∑–Ω—ã–µ dropout rates
    {'name': 'ResNet-Drop0.2', 'seed': 72, 'lr': 0.002, 'dropout': 0.2, 'hidden_dim': 64},
    {'name': 'ResNet-Drop0.4', 'seed': 82, 'lr': 0.002, 'dropout': 0.4, 'hidden_dim': 64},

    # –†–∞–∑–Ω—ã–µ hidden dimensions
    {'name': 'ResNet-Hid80', 'seed': 92, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 80},
    {'name': 'ResNet-Hid96', 'seed': 102, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 96},

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
    {'name': 'ResNet-Mix1', 'seed': 112, 'lr': 0.0015, 'dropout': 0.25, 'hidden_dim': 64},
    {'name': 'ResNet-Mix2', 'seed': 122, 'lr': 0.0025, 'dropout': 0.35, 'hidden_dim': 64},
    {'name': 'ResNet-Mix3', 'seed': 132, 'lr': 0.002, 'dropout': 0.3, 'hidden_dim': 72},
]

NUM_EPOCHS = 40
models = []
results = []

for i, config in enumerate(ensemble_config, 1):
    print(f'[{i}/{len(ensemble_config)}] Training {config["name"]} (seed={config["seed"]}, lr={config["lr"]}, dropout={config["dropout"]}, hidden_dim={config["hidden_dim"]})')
    print('-' * 70)

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed
    set_seed(config['seed'])

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ç–µ–∫—É—â–∏–º seed
    train_dataset = DataClass(split='train', download=False,
                            transform=train_transform, as_rgb=False)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = ResNetLikeCNN(
        num_classes=NUM_CLASSES,
        dropout=config['dropout'],
        hidden_dim=config['hidden_dim']
    ).to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config['lr'])

    # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
    best_val_acc = train_model_with_early_stopping(
        model, train_loader, val_loader, criterion, optimizer,
        NUM_EPOCHS, device, patience=15, verbose=True
    )

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    print(f'Best Val Acc: {best_val_acc:.2f}%')
    print(f'Test Acc: {test_acc:.2f}%')
    print()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    models.append(model)
    results.append({
        'name': config['name'],
        'seed': config['seed'],
        'lr': config['lr'],
        'dropout': config['dropout'],
        'hidden_dim': config['hidden_dim'],
        'best_val_acc': best_val_acc,
        'test_acc': test_acc
    })


# ===========================================================================
# –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ - Soft Voting
# ===========================================================================

print('=' * 70)
print('–ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–ï - SOFT VOTING (10 –ú–û–î–ï–õ–ï–ô)')
print('=' * 70)
print()

def ensemble_predict(models, dataloader, device):
    """–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ soft voting"""
    all_probs_ensemble = []
    all_labels = None

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for i, model in enumerate(models):
        print(f'Getting predictions from model {i+1}/{len(models)}...')
        probs, labels = get_predictions_proba(model, dataloader, device)
        all_probs_ensemble.append(probs)
        if all_labels is None:
            all_labels = labels

    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    avg_probs = np.mean(all_probs_ensemble, axis=0)
    predictions = np.argmax(avg_probs, axis=1)

    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
    accuracy = 100. * np.mean(predictions == all_labels)

    return accuracy, predictions, all_labels


# –ê–Ω—Å–∞–º–±–ª—å –Ω–∞ validation
print('Evaluating ensemble on validation set...')
val_ensemble_acc, _, _ = ensemble_predict(models, val_loader, device)
print(f'Validation Ensemble Accuracy: {val_ensemble_acc:.2f}%')
print()

# –ê–Ω—Å–∞–º–±–ª—å –Ω–∞ test
print('Evaluating ensemble on test set...')
test_ensemble_acc, test_preds, test_labels = ensemble_predict(models, test_loader, device)
print(f'Test Ensemble Accuracy: {test_ensemble_acc:.2f}%')
print()


# ===========================================================================
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# ===========================================================================

os.makedirs('results/experiments_results', exist_ok=True)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
ensemble_results = {
    'individual_models': results,
    'val_ensemble_acc': val_ensemble_acc,
    'test_ensemble_acc': test_ensemble_acc,
    'num_models': len(models),
    'ensemble_type': 'large_10_models'
}

with open('results/experiments_results/ensemble_large_results.json', 'w') as f:
    json.dump(ensemble_results, f, indent=2)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π
for i, model in enumerate(models):
    torch.save(model.state_dict(),
              f'results/experiments_results/ensemble_large_model_{i+1}.pth')

print('=' * 70)
print('–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´')
print('=' * 70)
print()

print('–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏:')
for res in results:
    print(f'  {res["name"]:<25} Test Acc: {res["test_acc"]:.2f}%')

print()
print(f'–°—Ä–µ–¥–Ω–µ–µ –ø–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º: {np.mean([r["test_acc"] for r in results]):.2f}%')
print(f'–õ—É—á—à–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è:     {max([r["test_acc"] for r in results]):.2f}%')
print()
print(f'–ê–Ω—Å–∞–º–±–ª—å 10 –º–æ–¥–µ–ª–µ–π:       {test_ensemble_acc:.2f}%')
print(f'–£–ª—É—á—à–µ–Ω–∏–µ:                 +{test_ensemble_acc - max([r["test_acc"] for r in results]):.2f}%')
print()

print('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:')
print(f'  MLP Ensemble (—Ä–∞–±–æ—Ç–∞ 1):          80.83%')
print(f'  ResNet-18 (—Å—Ç–∞—Ç—å—è):               87.7%')
print(f'  CNN Baseline (—Ä–∞–±–æ—Ç–∞ 2):          89.96%')
print(f'  CNN Ensemble 5 –º–æ–¥–µ–ª–µ–π:           91.16%')
print(f'  CNN Ensemble 10 –º–æ–¥–µ–ª–µ–π:          {test_ensemble_acc:.2f}%')
print()

if test_ensemble_acc > 92.0:
    print('üéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: >92% accuracy!')
else:
    print(f'üìä –¢–µ–∫—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {test_ensemble_acc:.2f}% (—Ü–µ–ª—å: >92%)')

print()
print('‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:')
print('  - results/experiments_results/ensemble_large_results.json')
print('  - results/experiments_results/ensemble_large_model_*.pth (10 –º–æ–¥–µ–ª–µ–π)')
print()

print('=' * 70)
print('–û–ë–£–ß–ï–ù–ò–ï –£–í–ï–õ–ò–ß–ï–ù–ù–û–ì–û –ê–ù–°–ê–ú–ë–õ–Ø –ó–ê–í–ï–†–®–ï–ù–û')
print('=' * 70)
