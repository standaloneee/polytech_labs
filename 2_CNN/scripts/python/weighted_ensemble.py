"""
Weighted Ensemble - Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ validation set Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð²ÐµÑÐ¾Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð¸Ñ (soft voting)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
import numpy as np
from scipy.optimize import minimize
import json
import medmnist
from medmnist import INFO
from tqdm import tqdm


# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°
if torch.cuda.is_available():
    device = torch.device('cuda')
elif torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')

print(f'Device: {device}\n')


# ===========================================================================
# ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ñ‚Ð° Ð¶Ðµ Ñ‡Ñ‚Ð¾ Ð¸ Ð² Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÑÑ…)
# ===========================================================================

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = F.relu(out)
        return out


class ResNetLikeCNN(nn.Module):
    def __init__(self, num_classes=11, dropout=0.3, hidden_dim=64):
        super(ResNetLikeCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, hidden_dim, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.pool = nn.MaxPool2d(2, 2)
        self.res_block1 = ResidualBlock(hidden_dim)
        self.res_block2 = ResidualBlock(hidden_dim)
        self.conv2 = nn.Conv2d(hidden_dim, 128, 3, padding=1, stride=2)
        self.bn2 = nn.BatchNorm2d(128)
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)
        x = self.res_block1(x)
        x = self.res_block2(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x


# ===========================================================================
# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
# ===========================================================================

data_flag = 'organcmnist'
info = INFO[data_flag]
NUM_CLASSES = len(info['label'])
BATCH_SIZE = 128

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

DataClass = getattr(medmnist, info['python_class'])

val_dataset = DataClass(split='val', download=False, transform=test_transform, as_rgb=False)
test_dataset = DataClass(split='test', download=False, transform=test_transform, as_rgb=False)

val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f'Val samples: {len(val_dataset)}')
print(f'Test samples: {len(test_dataset)}\n')


# ===========================================================================
# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
# ===========================================================================

print('=' * 70)
print('WEIGHTED ENSEMBLE')
print('=' * 70)
print()

# ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ
import os

model_files = []
ensemble_results_file = None

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ°ÐºÐ¾Ð¹ Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
if os.path.exists('results/experiments_results/ensemble_large_results.json'):
    ensemble_results_file = 'results/experiments_results/ensemble_large_results.json'
    model_pattern = 'results/experiments_results/ensemble_large_model_{}.pth'
elif os.path.exists('results/experiments_results/ensemble_results.json'):
    ensemble_results_file = 'results/experiments_results/ensemble_results.json'
    model_pattern = 'results/experiments_results/ensemble_model_{}.pth'
else:
    print('ÐžÑˆÐ¸Ð±ÐºÐ°: ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ!')
    exit(1)

# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
with open(ensemble_results_file, 'r') as f:
    ensemble_config = json.load(f)

num_models = ensemble_config['num_models']
print(f'Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {num_models} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸Ð· Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ...')

# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸
models = []
for i in range(1, num_models + 1):
    model_path = model_pattern.format(i)

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
    if 'individual_models' in ensemble_config and len(ensemble_config['individual_models']) >= i:
        model_info = ensemble_config['individual_models'][i-1]
        dropout = model_info.get('dropout', 0.3)
        hidden_dim = model_info.get('hidden_dim', 64)
    else:
        dropout = 0.3
        hidden_dim = 64

    model = ResNetLikeCNN(num_classes=NUM_CLASSES, dropout=dropout, hidden_dim=hidden_dim).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    models.append(model)
    print(f'  âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ {i}/{num_models}')

print(f'\nÐ£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(models)} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n')


# ===========================================================================
# ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð¾Ñ‚ Ð²ÑÐµÑ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
# ===========================================================================

def get_all_predictions(models, dataloader, device):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¾Ñ‚ Ð²ÑÐµÑ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    all_model_probs = []
    all_labels = None

    for i, model in enumerate(models):
        model.eval()
        model_probs = []
        labels = []

        with torch.no_grad():
            for images, label_batch in tqdm(dataloader, desc=f'Model {i+1}/{len(models)}', leave=False):
                images = images.to(device)
                outputs = model(images)
                probs = F.softmax(outputs, dim=1)
                model_probs.append(probs.cpu().numpy())
                labels.append(label_batch.squeeze().numpy())

        all_model_probs.append(np.vstack(model_probs))
        if all_labels is None:
            all_labels = np.concatenate(labels)

    # Shape: (num_models, num_samples, num_classes)
    return np.array(all_model_probs), all_labels


print('ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð½Ð° validation set...')
val_probs, val_labels = get_all_predictions(models, val_loader, device)
print(f'Val predictions shape: {val_probs.shape}\n')

print('ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð½Ð° test set...')
test_probs, test_labels = get_all_predictions(models, test_loader, device)
print(f'Test predictions shape: {test_probs.shape}\n')


# ===========================================================================
# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð½Ð° validation set
# ===========================================================================

print('ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð½Ð° validation set...')

def weighted_ensemble_accuracy(weights, probs, labels):
    """Ð’Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ accuracy Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²ÐµÑÐ¾Ð²"""
    # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð²ÐµÑÐ°
    weights = np.array(weights)
    weights = weights / weights.sum()

    # Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð¸Ðµ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹
    # probs shape: (num_models, num_samples, num_classes)
    weighted_probs = np.tensordot(weights, probs, axes=([0], [0]))

    # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
    predictions = np.argmax(weighted_probs, axis=1)

    # Accuracy
    accuracy = np.mean(predictions == labels)

    return accuracy


def objective(weights, probs, labels):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ negative accuracy)"""
    return -weighted_ensemble_accuracy(weights, probs, labels)


# ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÑÐ° (Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ)
initial_weights = np.ones(num_models) / num_models

# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ: Ð²ÐµÑÐ° Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¸ ÑÑƒÐ¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð² 1
constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}
bounds = [(0, 1) for _ in range(num_models)]

# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ
print('Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸...')
result = minimize(
    objective,
    initial_weights,
    args=(val_probs, val_labels),
    method='SLSQP',
    bounds=bounds,
    constraints=constraints,
    options={'maxiter': 1000, 'ftol': 1e-9}
)

optimal_weights = result.x
optimal_weights = optimal_weights / optimal_weights.sum()  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼

print('\nÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÑÐ°:')
for i, w in enumerate(optimal_weights):
    print(f'  ÐœÐ¾Ð´ÐµÐ»ÑŒ {i+1}: {w:.4f}')

# Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð²ÐµÑÐ°Ð¼Ð¸
val_acc_uniform = weighted_ensemble_accuracy(initial_weights, val_probs, val_labels) * 100
val_acc_weighted = weighted_ensemble_accuracy(optimal_weights, val_probs, val_labels) * 100

print(f'\nValidation Accuracy:')
print(f'  Uniform weights:  {val_acc_uniform:.2f}%')
print(f'  Optimal weights:  {val_acc_weighted:.2f}%')
print(f'  Improvement:      +{val_acc_weighted - val_acc_uniform:.2f}%')


# ===========================================================================
# Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð²ÐµÑÐ°Ð¼Ð¸
# ===========================================================================

print('\nÐ¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° test set...')

test_acc_uniform = weighted_ensemble_accuracy(initial_weights, test_probs, test_labels) * 100
test_acc_weighted = weighted_ensemble_accuracy(optimal_weights, test_probs, test_labels) * 100

print(f'\nTest Accuracy:')
print(f'  Uniform weights (Soft Voting):  {test_acc_uniform:.2f}%')
print(f'  Optimal weights (Weighted):      {test_acc_weighted:.2f}%')
print(f'  Improvement:                     +{test_acc_weighted - test_acc_uniform:.2f}%')


# ===========================================================================
# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
# ===========================================================================

results = {
    'num_models': num_models,
    'optimal_weights': optimal_weights.tolist(),
    'val_acc_uniform': val_acc_uniform,
    'val_acc_weighted': val_acc_weighted,
    'test_acc_uniform': test_acc_uniform,
    'test_acc_weighted': test_acc_weighted,
    'improvement_val': val_acc_weighted - val_acc_uniform,
    'improvement_test': test_acc_weighted - test_acc_uniform
}

output_file = 'results/experiments_results/weighted_ensemble_results.json'
with open(output_file, 'w') as f:
    json.dump(results, f, indent=2)

print(f'\nâœ“ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: {output_file}')

print('\n' + '=' * 70)
if test_acc_weighted > 92.0:
    print('ðŸŽ‰ Ð¦Ð•Ð›Ð¬ Ð”ÐžÐ¡Ð¢Ð˜Ð“ÐÐ£Ð¢Ð: >92% accuracy!')
else:
    print(f'ðŸ“Š Ð¢ÐµÐºÑƒÑ‰Ð¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {test_acc_weighted:.2f}% (Ñ†ÐµÐ»ÑŒ: >92%)')
print('=' * 70)
