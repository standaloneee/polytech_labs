Experiment,Test_Acc,Val_Acc,Params,Dropout,Weight_Decay,LR,BatchNorm,Activation
Batch Norm: BN + Dropout 0.3,78.33495618305744,91.22073578595318,109835,0.3,0.0,0.001,True,relu
Batch Norm: With BN,77.28821811100292,89.84113712374582,109835,0.0,0.0,0.001,True,relu
Architecture: Large,76.92307692307692,89.04682274247492,235275,0.0,0.0,0.001,False,relu
Regularization: Dropout 0.3,76.61879259980526,90.13377926421404,109451,0.3,0.0,0.001,False,relu
Regularization: Dropout 0.3 + L2 1e-4,76.48490749756573,90.25919732441471,109451,0.3,0.0001,0.001,False,relu
Regularization: L2 1e-3,76.43622200584225,89.25585284280936,109451,0.0,0.001,0.001,False,relu
Regularization: Dropout 0.2,76.42405063291139,90.25919732441471,109451,0.2,0.0,0.001,False,relu
Activation: elu,76.39970788704966,89.21404682274247,109451,0.0,0.0,0.001,False,elu
Regularization: L2 1e-4,75.38948393378773,89.33946488294315,109451,0.0,0.0001,0.001,False,relu
LR: LR 0.001 + StepLR,75.3286270691334,89.54849498327759,109451,0.0,0.0,0.001,False,relu
Regularization: Dropout 0.5,75.2190847127556,89.29765886287625,109451,0.5,0.0,0.001,False,relu
LR: LR 0.001 + CosineAnnealing,75.18257059396299,89.38127090301003,109451,0.0,0.0,0.001,False,relu
Architecture: Very Deep,75.18257059396299,89.38127090301003,127691,0.0,0.0,0.001,False,relu
Architecture: Deep,74.9634858812074,88.79598662207358,125963,0.0,0.0,0.001,False,relu
LR: LR 0.001,74.84177215189874,89.54849498327759,109451,0.0,0.0,0.001,False,relu
Batch Norm: Without BN,74.84177215189874,89.54849498327759,109451,0.0,0.0,0.001,False,relu
Activation: relu,74.84177215189874,89.54849498327759,109451,0.0,0.0,0.001,False,relu
Architecture: Baseline,74.84177215189874,89.54849498327759,109451,0.0,0.0,0.001,False,relu
Regularization: No Reg,74.84177215189874,89.54849498327759,109451,0.0,0.0,0.001,False,relu
LR: LR 0.01,74.76874391431353,88.75418060200668,109451,0.0,0.0,0.01,False,relu
Activation: leaky_relu,74.73222979552094,88.87959866220736,109451,0.0,0.0,0.001,False,leaky_relu
LR: LR 0.0001,73.77069133398247,88.79598662207358,109451,0.0,0.0,0.0001,False,relu
Architecture: Small,72.87000973709834,89.1304347826087,52683,0.0,0.0,0.001,False,relu
Activation: tanh,71.93281402142162,88.21070234113712,109451,0.0,0.0,0.001,False,tanh
