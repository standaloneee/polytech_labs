{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнительный анализ моделей аспектного анализа тональности русских отзывов\n",
    "\n",
    "## Цель исследования\n",
    "Провести сравнительный анализ готовых моделей для анализа тональности русских отзывов с HuggingFace.\n",
    "\n",
    "## Модели для сравнения:\n",
    "1. **blanchefort/rubert-base-cased-sentiment-rurewiews** - RuBERT на отзывах RuReviews\n",
    "2. **sismetanin/rubert-ru-sentiment-rureviews** - RuBERT на отзывах e-commerce\n",
    "3. **seara/rubert-tiny2-russian-sentiment** - Легковесная версия RuBERT\n",
    "4. **sismetanin/xlm_roberta_base-ru-sentiment-rureviews** - XLM-RoBERTa для русского языка\n",
    "\n",
    "## Метрики оценки:\n",
    "- Accuracy\n",
    "- F1-score (macro/weighted)\n",
    "- Precision и Recall\n",
    "- Время инференса\n",
    "- Размер модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "# !pip install transformers torch datasets pandas scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import ssl\n",
    "import urllib.request\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Отключение проверки SSL сертификатов (для корпоративных сетей)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Для requests (используется HuggingFace)\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "# Настройка отображения\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Пути к файлам\n",
    "RESEARCH_DIR = Path.cwd()\n",
    "print(f\"Рабочая директория: {RESEARCH_DIR}\")\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Используемое устройство: {device}')\n",
    "print(\"\\nПримечание: SSL проверка отключена для загрузки моделей\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка тестовых данных\n",
    "\n",
    "Создадим набор тестовых русских отзывов с разными аспектами и тональностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем тестовые отзывы с известной тональностью\n",
    "test_reviews = [\n",
    "    # Позитивные отзывы\n",
    "    {\"text\": \"Отличный отель! Номера чистые, персонал вежливый, завтрак превосходный.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Прекрасный ресторан, вкусная еда, быстрое обслуживание. Обязательно вернусь!\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Товар пришел быстро, качество отличное, упаковка хорошая. Рекомендую!\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Замечательный ноутбук, быстрый, легкий, батарея держит долго.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Очень довольна покупкой! Платье красивое, ткань приятная, сидит идеально.\", \"label\": \"POSITIVE\"},\n",
    "    \n",
    "    # Негативные отзывы\n",
    "    {\"text\": \"Ужасный отель! Грязные номера, невежливый персонал, плохой завтрак.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Разочарован покупкой. Товар низкого качества, долгая доставка, плохая упаковка.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Не рекомендую этот ресторан. Еда невкусная, обслуживание медленное, цены завышены.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Ноутбук постоянно зависает, батарея быстро разряжается, очень тяжелый.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Платье пришло не того размера, ткань дешевая, швы кривые. Деньги на ветер.\", \"label\": \"NEGATIVE\"},\n",
    "    \n",
    "    # Нейтральные отзывы\n",
    "    {\"text\": \"Обычный отель, ничего особенного. Номера стандартные, персонал нормальный.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Товар соответствует описанию. Доставка в срок.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Ресторан как ресторан, еда средняя, цены приемлемые.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Ноутбук подходит для базовых задач. Ничего выдающегося.\", \"label\": \"NEUTRAL\"},\n",
    "    \n",
    "    # Смешанные (аспектные) отзывы\n",
    "    {\"text\": \"Отель хороший, но завтрак разочаровал. Номера чистые, но персонал грубый.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Еда в ресторане отличная, но ждали очень долго. Атмосфера приятная, но цены высокие.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Ноутбук мощный, но очень шумный. Экран яркий, но батарея слабая.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Качество товара хорошее, но доставка задержалась на неделю.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Платье красивое, но маломерит. Ткань качественная, но дорого.\", \"label\": \"NEUTRAL\"},\n",
    "    \n",
    "    # Дополнительные позитивные\n",
    "    {\"text\": \"Превосходное качество обслуживания! Всё на высшем уровне.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Лучший отель в городе! Всем рекомендую.\", \"label\": \"POSITIVE\"},\n",
    "    \n",
    "    # Дополнительные негативные\n",
    "    {\"text\": \"Полное разочарование. Не советую никому.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Потраченные деньги жалко. Очень плохо.\", \"label\": \"NEGATIVE\"},\n",
    "]\n",
    "\n",
    "# Преобразуем в DataFrame\n",
    "df_test = pd.DataFrame(test_reviews)\n",
    "print(f\"Создано тестовых отзывов: {len(df_test)}\")\n",
    "print(f\"\\nРаспределение по классам:\")\n",
    "print(df_test['label'].value_counts())\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка моделей\n",
    "\n",
    "Загружаем все модели для сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей для тестирования\n",
    "model_names = [\n",
    "    \"blanchefort/rubert-base-cased-sentiment-rurewiews\",\n",
    "    \"sismetanin/rubert-ru-sentiment-rureviews\",\n",
    "    \"seara/rubert-tiny2-russian-sentiment\",\n",
    "    \"sismetanin/xlm_roberta_base-ru-sentiment-rureviews\",\n",
    "]\n",
    "\n",
    "# Директория для локального хранения моделей\n",
    "MODELS_CACHE_DIR = RESEARCH_DIR / 'models_cache'\n",
    "MODELS_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Модели будут сохраняться в: {MODELS_CACHE_DIR}\\n\")\n",
    "\n",
    "# Словарь для хранения pipelines\n",
    "models = {}\n",
    "\n",
    "print(\"Загрузка моделей...\\n\")\n",
    "for model_name in model_names:\n",
    "    print(f\"Загружается: {model_name}\")\n",
    "    \n",
    "    # Короткое имя для локального хранения\n",
    "    short_name = model_name.split('/')[-1]\n",
    "    local_model_path = MODELS_CACHE_DIR / short_name\n",
    "    \n",
    "    try:\n",
    "        # Проверяем, есть ли модель локально\n",
    "        if local_model_path.exists() and (local_model_path / 'config.json').exists():\n",
    "            print(f\"  Найдена локальная копия, загружаем из {local_model_path}\")\n",
    "            pipe = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=str(local_model_path),\n",
    "                device=0 if device == 'cuda' else -1\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  Скачивание с HuggingFace...\")\n",
    "            pipe = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=model_name,\n",
    "                device=0 if device == 'cuda' else -1\n",
    "            )\n",
    "            # Сохраняем модель локально\n",
    "            print(f\"  Сохранение в {local_model_path}\")\n",
    "            pipe.model.save_pretrained(local_model_path)\n",
    "            pipe.tokenizer.save_pretrained(local_model_path)\n",
    "        \n",
    "        models[short_name] = pipe\n",
    "        print(f\"  Загружена успешно\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Ошибка загрузки: {e}\\n\")\n",
    "\n",
    "print(f\"Всего загружено моделей: {len(models)}\")\n",
    "print(f\"\\nМодели сохранены в: {MODELS_CACHE_DIR}\")\n",
    "print(\"При следующем запуске будут загружены из локального кэша\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Получение информации о моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Информация о моделях\n",
    "model_info = []\n",
    "\n",
    "for short_name, pipe in models.items():\n",
    "    model = pipe.model\n",
    "    # Подсчет параметров\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    model_info.append({\n",
    "        'Модель': short_name,\n",
    "        'Всего параметров': f\"{total_params:,}\",\n",
    "        'Обучаемых параметров': f\"{trainable_params:,}\",\n",
    "        'Классы': pipe.model.config.id2label if hasattr(pipe.model.config, 'id2label') else 'N/A'\n",
    "    })\n",
    "\n",
    "df_info = pd.DataFrame(model_info)\n",
    "print(\"Информация о загруженных моделях:\\n\")\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование моделей\n",
    "\n",
    "Прогоняем все отзывы через каждую модель и собираем результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для нормализации меток\n",
    "def normalize_label(label):\n",
    "    \"\"\"Нормализует различные форматы меток к стандартному виду\"\"\"\n",
    "    label = str(label).upper()\n",
    "    \n",
    "    if 'POSITIVE' in label or 'LABEL_2' in label or label == '2':\n",
    "        return 'POSITIVE'\n",
    "    elif 'NEGATIVE' in label or 'LABEL_0' in label or label == '0':\n",
    "        return 'NEGATIVE'\n",
    "    elif 'NEUTRAL' in label or 'LABEL_1' in label or label == '1':\n",
    "        return 'NEUTRAL'\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "# Функция для тестирования модели\n",
    "def test_model(pipe, texts, batch_size=8):\n",
    "    \"\"\"Тестирует модель на списке текстов\"\"\"\n",
    "    predictions = []\n",
    "    scores = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Обработка батчей\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = pipe(batch)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        for result in results:\n",
    "            predictions.append(normalize_label(result['label']))\n",
    "            scores.append(result['score'])\n",
    "            inference_times.append(batch_time / len(batch))\n",
    "    \n",
    "    return predictions, scores, inference_times\n",
    "\n",
    "# Тестируем все модели\n",
    "results = {}\n",
    "texts = df_test['text'].tolist()\n",
    "true_labels = df_test['label'].tolist()\n",
    "\n",
    "print(\"Тестирование моделей на отзывах...\\n\")\n",
    "for model_name, pipe in models.items():\n",
    "    print(f\"\\nТестирование: {model_name}\")\n",
    "    predictions, scores, times = test_model(pipe, texts)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'predictions': predictions,\n",
    "        'scores': scores,\n",
    "        'times': times,\n",
    "        'avg_time': np.mean(times)\n",
    "    }\n",
    "    \n",
    "    print(f\"Средняя время инференса: {results[model_name]['avg_time']:.4f} сек/отзыв\")\n",
    "\n",
    "print(\"\\nВсе модели протестированы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Оценка качества моделей\n",
    "\n",
    "Вычисляем метрики для каждой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисляем метрики для каждой модели\n",
    "metrics_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    predictions = result['predictions']\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro метрики\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics_data.append({\n",
    "        'Модель': model_name,\n",
    "        'Accuracy': f\"{accuracy:.4f}\",\n",
    "        'Precision (weighted)': f\"{precision:.4f}\",\n",
    "        'Recall (weighted)': f\"{recall:.4f}\",\n",
    "        'F1-score (weighted)': f\"{f1:.4f}\",\n",
    "        'F1-score (macro)': f\"{f1_macro:.4f}\",\n",
    "        'Avg Inference Time (s)': f\"{result['avg_time']:.4f}\"\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(\"Метрики качества моделей:\\n\")\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Детальные отчеты по классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Детальные отчеты для каждой модели\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Модель: {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    predictions = result['predictions']\n",
    "    print(classification_report(true_labels, predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрицы ошибок для всех моделей\n",
    "n_models = len(models)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    if idx < len(axes):\n",
    "        cm = confusion_matrix(true_labels, result['predictions'], \n",
    "                             labels=['NEGATIVE', 'NEUTRAL', 'POSITIVE'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                   xticklabels=['NEGATIVE', 'NEUTRAL', 'POSITIVE'],\n",
    "                   yticklabels=['NEGATIVE', 'NEUTRAL', 'POSITIVE'])\n",
    "        axes[idx].set_title(f'Confusion Matrix: {model_name}', fontsize=10)\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESEARCH_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Матрицы ошибок сохранены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение метрик\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# График 1: Accuracy и F1-score\n",
    "metrics_comparison = []\n",
    "for model_name, result in results.items():\n",
    "    predictions = result['predictions']\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    _, _, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'F1 (weighted)': f1_weighted,\n",
    "        'F1 (macro)': f1_macro\n",
    "    })\n",
    "\n",
    "df_comp = pd.DataFrame(metrics_comparison)\n",
    "df_comp.set_index('Model')[['Accuracy', 'F1 (weighted)', 'F1 (macro)']].plot(\n",
    "    kind='bar', ax=axes[0], rot=45\n",
    ")\n",
    "axes[0].set_title('Сравнение метрик качества', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Значение метрики')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim([0, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# График 2: Время инференса\n",
    "times_data = [(name, result['avg_time']) for name, result in results.items()]\n",
    "models_names, avg_times = zip(*times_data)\n",
    "axes[1].bar(range(len(models_names)), avg_times, color='coral')\n",
    "axes[1].set_xticks(range(len(models_names)))\n",
    "axes[1].set_xticklabels(models_names, rotation=45, ha='right')\n",
    "axes[1].set_title('Среднее время инференса', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Время (секунды)')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESEARCH_DIR / 'metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Сравнительные графики сохранены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Анализ примеров предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем таблицу с примерами предсказаний\n",
    "examples_df = df_test.copy()\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    examples_df[f'{model_name}_pred'] = result['predictions']\n",
    "    examples_df[f'{model_name}_score'] = [f\"{s:.3f}\" for s in result['scores']]\n",
    "\n",
    "print(\"Примеры предсказаний всех моделей:\\n\")\n",
    "examples_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ расхождений между моделями\n",
    "print(\"Анализ расхождений между моделями:\\n\")\n",
    "\n",
    "# Найдем отзывы где модели не согласны\n",
    "model_names_list = list(results.keys())\n",
    "disagreements = []\n",
    "\n",
    "for idx, row in examples_df.iterrows():\n",
    "    predictions = [row[f'{model}_pred'] for model in model_names_list]\n",
    "    \n",
    "    # Если не все предсказания одинаковые\n",
    "    if len(set(predictions)) > 1:\n",
    "        disagreements.append({\n",
    "            'Текст': row['text'][:60] + '...',\n",
    "            'Истинная метка': row['label'],\n",
    "            **{f'{model}': row[f'{model}_pred'] for model in model_names_list}\n",
    "        })\n",
    "\n",
    "if disagreements:\n",
    "    df_disagree = pd.DataFrame(disagreements)\n",
    "    print(f\"Найдено {len(df_disagree)} отзывов с расхождениями в предсказаниях:\\n\")\n",
    "    display(df_disagree)\n",
    "else:\n",
    "    print(\"Все модели согласны во всех предсказаниях!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Итоговые выводы и рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим лучшую модель по разным критериям\n",
    "best_accuracy = max(metrics_comparison, key=lambda x: x['Accuracy'])\n",
    "best_f1 = max(metrics_comparison, key=lambda x: x['F1 (weighted)'])\n",
    "fastest = min(times_data, key=lambda x: x[1])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ СРАВНИТЕЛЬНОГО АНАЛИЗА\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nЛучшая accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})\")\n",
    "print(f\"Лучший F1-score: {best_f1['Model']} ({best_f1['F1 (weighted)']:.4f})\")\n",
    "print(f\"Самая быстрая: {fastest[0]} ({fastest[1]:.4f} сек/отзыв)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nРЕКОМЕНДАЦИИ:\")\n",
    "print(\"- Для максимальной точности: используйте модели на основе RuBERT-base\")\n",
    "print(\"- Для быстрого инференса: используйте RuBERT-tiny2\")\n",
    "print(\"- Для продакшн-систем: баланс между RuBERT и XLM-RoBERTa\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем результаты в CSV\n",
    "df_metrics.to_csv(RESEARCH_DIR / 'model_metrics.csv', index=False)\n",
    "examples_df.to_csv(RESEARCH_DIR / 'predictions.csv', index=False)\n",
    "\n",
    "print(\"Результаты сохранены в файлы:\")\n",
    "print(\"  - model_metrics.csv\")\n",
    "print(\"  - predictions.csv\")\n",
    "print(\"  - confusion_matrices.png\")\n",
    "print(\"  - metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ожидаемые результаты и выводы\n",
    "\n",
    "### Цель анализа\n",
    "\n",
    "В результате исследования предполагается определить, какие модели демонстрируют наиболее высокую эффективность при анализе русскоязычных отзывов, а также выявить ограничения существующих решений.\n",
    "\n",
    "### Ожидаемые результаты\n",
    "\n",
    "#### 1. Сравнительная таблица результатов\n",
    "\n",
    "По итогам тестирования ожидается получить следующие данные:\n",
    "\n",
    "**Метрики качества классификации:**\n",
    "- Accuracy (точность) - общая доля правильных предсказаний\n",
    "- F1-score (weighted/macro) - гармоническое среднее precision и recall\n",
    "- Precision - точность положительных предсказаний\n",
    "- Recall - полнота обнаружения классов\n",
    "\n",
    "**Метрики производительности:**\n",
    "- Время инференса на один отзыв\n",
    "- Размер модели (количество параметров)\n",
    "\n",
    "#### 2. Выявленные паттерны\n",
    "\n",
    "**Ожидается, что:**\n",
    "\n",
    "- **RuBERT-base модели** покажут высокую точность благодаря предобучению на русскоязычных корпусах\n",
    "- **RuBERT-tiny** будет быстрее, но может уступать в точности\n",
    "- **XLM-RoBERTa** продемонстрирует баланс между качеством и универсальностью\n",
    "- Модели могут испытывать трудности с **аспектными (смешанными) отзывами**, где присутствуют как позитивные, так и негативные оценки\n",
    "\n",
    "#### 3. Ограничения текущих решений\n",
    "\n",
    "**Возможные проблемы:**\n",
    "\n",
    "1. **Аспектный анализ:** Стандартные модели sentiment analysis классифицируют текст целиком, не выделяя отдельные аспекты (качество, сервис, цена)\n",
    "\n",
    "2. **Контекстная чувствительность:** Отзывы типа \"Отель хороший, НО завтрак плохой\" могут классифицироваться неоднозначно\n",
    "\n",
    "3. **Доменная специфика:** Модели, обученные на одном типе отзывов (e-commerce), могут хуже работать на других доменах (отели, рестораны)\n",
    "\n",
    "4. **Размер модели vs качество:** Компактные модели быстрее, но теряют в точности\n",
    "\n",
    "### Аналитическое заключение\n",
    "\n",
    "**По результатам исследования будет сформулировано:**\n",
    "\n",
    "1. **Рейтинг моделей** по различным критериям:\n",
    "   - Лучшая по точности\n",
    "   - Оптимальная по соотношению скорость/качество\n",
    "   - Рекомендуемая для production-систем\n",
    "\n",
    "2. **Рекомендации по применению:**\n",
    "   - Для каких задач какая модель подходит лучше\n",
    "   - Когда стоит использовать легковесные модели\n",
    "   - В каких случаях нужны более тяжелые архитектуры\n",
    "\n",
    "3. **Направления улучшения:**\n",
    "   - Дообучение (fine-tuning) на специфичных данных\n",
    "   - Использование ансамблей моделей\n",
    "   - Применение специализированных моделей для аспектного анализа\n",
    "\n",
    "### Практическая значимость\n",
    "\n",
    "Результаты исследования помогут:\n",
    "\n",
    "- **Разработчикам** выбрать оптимальную модель для своих задач\n",
    "- **Исследователям** понять текущее состояние sentiment analysis для русского языка\n",
    "- **Бизнесу** оценить возможности автоматизации анализа отзывов\n",
    "\n",
    "### Воспроизводимость\n",
    "\n",
    "Все эксперименты выполняются на фиксированном наборе тестовых отзывов, что обеспечивает:\n",
    "- Объективность сравнения\n",
    "- Возможность воспроизведения результатов\n",
    "- Прозрачность методологии оценки\n",
    "\n",
    "---\n",
    "\n",
    "**Итоговый вывод:** Сравнительный анализ готовых моделей позволит сформировать практические рекомендации по выбору инструментов для анализа тональности русскоязычных отзывов и определить направления дальнейших исследований в этой области."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
